{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of cat images with Multi-Layer (Deep) Neural Network\n",
    "\n",
    "This is the second part of Programming Assignment 2 (PA2)! \n",
    "\n",
    "In this part, you will use the functions you just implemented in the first part and build a deep network to classify cat images (being cat or non-cat). You are expected to notice an improvement in accuracy relative to your previous logistic regression implementation from PA1.  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" markdown=\"1\">\n",
    "    <strong>After this assignment, you should be able to:</strong>\n",
    "    <ul> \n",
    "        <li>Build and apply a deep (multi-layer) neural network for supervised learning.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the packages that we will use in this assignment first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from utils2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset\n",
    "\n",
    "Use the same \"Cat vs non-Cat\" dataset from the PA1 (previous assignment). The model you had built there had 70% test accuracy. By using more layers, your new model should perform better!\n",
    "\n",
    "**Problem Statement**: You are given a dataset containing:\n",
    "  - a training set of m_train images labelled as cat (with label = 1) or non-cat (with label = 0)\n",
    "  - a test set of m_test images where each image is labelled as being a cat image or a non-cat image.\n",
    "  - each image is of shape (num_px, num_px, 3) where 3 is for the 3 color channels (RGB).\n",
    "\n",
    "Let's remember the dataset first. Below, load the dataset first and then show a sample image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show you a sample image from the training set. Feel free to change the index and re-run the cell to see different images from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 9\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". This is a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, reshape (i.e., convert the image data into a vector) and standardize (normalize) all the images before feeding them to the network. The code for that purpose is given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12,288$ equals to: $64 \\times 64 \\times 3$ which is the size of one image vector after reshaped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Architecture of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a deep neural network to distinguish cat images from non-cat images.\n",
    "\n",
    "In this part of the assignment, you will build two different models:\n",
    "- A 2-layer neural network\n",
    "- An L-layer deep neural network\n",
    "\n",
    "You will then compare the performance of these models. \n",
    "\n",
    "### 3.1 - Two-layer neural network\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 2</u>: Two-layer neural network. <br> The complete NN architecture can be summarized as: ***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***. </center></caption>\n",
    "\n",
    "<u>Detailed Architecture of figure 2</u>:\n",
    "- The input is a (64,64,3) dimensional image which is flattened to a vector of dims: $(12288,1)$. \n",
    "- The corresponding (flattened image) vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- Also add a bias term and then pass that from relu to get the following vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n",
    "- Repeat the same processes describe above for the next layer:\n",
    "- You multiply the resulting vector by $W^{[2]}$ and add your bias. \n",
    "- Finally, you take the sigmoid of the result. If the value is greater than 0.5, you classify that image as cat.\n",
    "\n",
    "### 3.2 - L-layer deep neural network\n",
    "\n",
    "It is easier to represent larger networks (L-layer deep neural network) in a more abstract way as shown below:\n",
    "\n",
    "<img src=\"images/LlayerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Figure 3</u>: L-layer neural network. <br> The L-layer NN architecture (the complete model) can be summarized as: ***[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption>\n",
    "\n",
    "<u>Detailed Architecture of figure 3</u>:\n",
    "- The input is a (64,64,3) dimensional image which is flattened into a vector of dims: (12288,1).\n",
    "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then first multiplied by the weight matrix $W^{[1]}$ and then added to the bias: $b^{[1]}$. The result is called the linear unit output (z).\n",
    "- Next, you apply the relu on the linear unit output. This process could be repeated several times for each $(W^{[l]}, b^{[l]})$ depending on the model architecture (if you use relu for activation).\n",
    "- Finally, you take the sigmoid of the final linear unit output. If it is greater than 0.5, you classify that corresponding image as cat.\n",
    "\n",
    "### 3.3 - General methodology\n",
    "\n",
    "The following first two steps summarize the processes for 'training' a neural network and the last step (step 3) is used for 'testing' a neural network:\n",
    "    1. Initialize parameters / Define hyperparameters\n",
    "    2. Loop for num_iterations:\n",
    "        a. Forward propagation\n",
    "        b. Compute cost function\n",
    "        c. Backward propagation\n",
    "        d. Update parameters (using parameters, and grads from backprop) \n",
    "    3. Use trained parameters to predict labels\n",
    "\n",
    "Let's now implement those two models next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Two-layer neural network\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 1</h2>\n",
    "<p> By using the helper functions you have already implemented in the previous part, build a 2-layer neural network with the following structure (model): `linear -> relu -> linear -> sigmoid`.</p>\n",
    "</div>\n",
    "\n",
    "The functions you may need and their inputs are:\n",
    "\n",
    "```python\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    # ...\n",
    "    return parameters \n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    # ...\n",
    "    return A, cache\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    # ...\n",
    "    return cost\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    # ...\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    # ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = None\n",
    "        A2, cache2 = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = None\n",
    "        dA0, dW1, db1 = None\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {:4d}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to train your network. Notice how long it takes to run your model and train it. The cost should be decreasing. It may take up to 5 minutes to run 2500 iterations. Check if the \"Cost after iteration 0\" matches the expected output below. If not, click on the square (⬛) on the upper bar of this notebook to stop the cell and fix your error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Cost after iteration    0: 0.6930497356599888\n",
    "Cost after iteration  100: 0.6464320953428849\n",
    "...\n",
    "Cost after iteration 2400: 0.048554785628770206\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good that you built a vectorized implementation! Otherwise, it might have taken 10 times longer to train that NN.\n",
    "\n",
    "Now, you can use the trained parameters to classify all the images in the dataset. To see your predictions on both the training and test sets, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Accuracy: 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Accuracy: 0.72\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is typically called \"early stopping\" and we talked about that in the class already. Early stopping is a way to avoid overfitting.\n",
    "\n",
    "Great! Your 2-layer neural network has better performance (72%) than the logistic regression implementation from the last assignment (70%). Let's see if a $L$-layer model (where $L>2$) could do even better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - L-layer Neural Network\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h2>Exercise 2</h2>\n",
    "<p> Use the helper functions you have already implemented and build an $L$-layer neural network with the following structure (model): `[linear -> relu]`$\\times$`(L-1) -> linear -> sigmoid`.</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "The functions you may need and their inputs are:\n",
    "```python\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    # ...\n",
    "    return parameters\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    # ...\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    # ...\n",
    "    return cost\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    # ...\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    # ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [linear -> relu]*(L-1) -> linear -> sigmoid.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = None\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = None\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = None\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {:4d}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now train the model as a 4-layer neural network ($L=4$). \n",
    "\n",
    "Run the cell below and train your 4-layer model. The cost should decrease at each iteration. It may take up to 5 minutes to run 2500 iterations. Check if the \"Cost after iteration 0\" matches the expected output below, if not, click on the square (⬛) on the upper bar of this notebook to stop the cell and debug/fix your error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Cost after iteration    0: 0.771749\n",
    "Cost after iteration  100: 0.672053\n",
    "...\n",
    "Cost after iteration 2400: 0.092878\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Accuracy: 0.985645933014\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Accuracy: 0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that your 4-layer neural network has even better performance (80%) than your 2-layer neural network (72%) on the same test set. \n",
    "\n",
    "You can try different number of layers in your L-model NN to see if you can get even better numbers. But submit your results for  the case given above ($L=4$ case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6) Results Analysis\n",
    "\n",
    "Now, let's have a look at some misclassified images by the L-layer model. The code below will show a few mislabeled images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few types of images where the model tends to perform poorly in classification may include:** \n",
    "- Cat body being in an unusual position\n",
    "- Cat appearing against a background of a similar color\n",
    "- Unusual colors and species\n",
    "- Camera angle\n",
    "- Different brightness levels in the picture\n",
    "- Scale variation (cat being very large or small in the image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment as well. You can use your own image and see the output of your model. To do that:\n",
    "  1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go to the folder where you started your jupyter notebook (remember that, in jupyter notebook, you cannot go to higher folders).\n",
    "  2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "  3. Edit the following code to include your image file's name \n",
    "  4. Run the code and see if the algorithm is classifying that image correctly (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## START CODE HERE ##\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "my_image = my_image/255.\n",
    "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
